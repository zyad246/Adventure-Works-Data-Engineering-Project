# Adventure Works Data Engineering Project - Azure Implementation

## Overview  
This project follows the **Medallion Architecture (Bronze, Silver, Gold)** to ingest, transform, and analyze the **Adventure Works dataset** using:  
- **Azure Data Factory**
- **Azure Data Lake**
- **Azure Databricks**
- **Azure Synapse Analytics**

Each phase serves a distinct purpose:  
- **Bronze Layer** â†’ Raw data ingestion  
- **Silver Layer** â†’ Data transformation & cleansing  
- **Gold Layer** â†’ Data aggregation & analytics  

---

## ğŸš€ **Bronze Layer: Raw Data Ingestion**  
### ğŸ¯ **Objective**  
Extract data from **GitHub API**, ingest it into **Azure Data Lake (Bronze container)**, and create an **ETL pipeline** in **Azure Data Factory (ADF)**.

### ğŸ›  **Implementation Steps**  
#### ğŸ”¹ Extract Data from GitHub API  
âœ… Used **HTTP Linked Service** to fetch raw dataset from the **GitHub repository**.  
âœ… Created a **Lookup Activity** in **ADF** to fetch file details dynamically.   

#### ğŸ”¹ Ingest Data into Azure Data Lake  
âœ… Used **ForEach Activity** in ADF to iterate over files and load them into **Bronze container**.  
âœ… Created a **dynamic dataset** in ADF using parameters for folder and file names.  


#### ğŸ”¹ Storage Structure in Azure Data Lake (Bronze Container)  
âœ… Data stored in **folders per dataset** in **Azure Data Lake Gen2**.  
  

### ğŸ“š **Topics Covered**  
âœ”ï¸ Working with APIs  
âœ”ï¸ Understanding Azure Data Factory & Linked Services  
âœ”ï¸ Static & Dynamic Pipelines in ADF  
âœ”ï¸ Azure Data Lake Gen2  

---

## âš¡ **Silver Layer: Data Transformation in Azure Databricks**  
### ğŸ¯ **Objective**  
Transform raw data in **Azure Databricks** using **Apache Spark**, store cleaned data in **Parquet format**, and save it into the **Silver container** in Data Lake.

### ğŸ›  **Implementation Steps**  
#### ğŸ”¹ Set Up Azure Databricks & Cluster  
âœ… Created **Databricks Workspace** and configured **Service Principal** for access to Data Lake.  
âœ… Configured **Role Assignments** to access the storage.  

#### ğŸ”¹ Load & Transform Data in Databricks  
âœ… Read raw data from **Bronze storage** using **Spark DataFrames**.  
âœ… Applied **Data Cleansing & Transformation** (e.g., handling null values, converting data types).  
âœ… Saved transformed data in **Parquet format** in the **Silver storage**.   

#### ğŸ”¹ Storage Structure in Azure Data Lake (Silver Container)  
âœ… Transformed data stored in **organized folders** in **Parquet format**. 

### ğŸ“š **Topics Covered**  
âœ”ï¸ Azure Databricks Cluster setup & configuration  
âœ”ï¸ Role Assignments with Data Lake  
âœ”ï¸ Working with Apache Spark  
âœ”ï¸ Data Cleansing & Transformation in Spark  

---

## ğŸ† **Gold Layer: Data Analysis with Azure Synapse Analytics**  
### ğŸ¯ **Objective**  
Extract transformed data from **Silver storage** into **Azure Synapse Analytics**, create a **Serverless SQL Database**, and implement an **abstraction layer using Views & External Tables**.

### ğŸ›  **Implementation Steps**  
#### ğŸ”¹ Set Up Azure Synapse & Create External Tables  
âœ… Created **Azure Synapse Workspace**.  
âœ… Configured **External Data Source** pointing to the **Silver storage**.  
âœ… Defined **External File Formats** for reading **Parquet files**.  
âœ… Created **External Tables** in Synapse for each dataset.  

#### ğŸ”¹ Implement Data Lakehouse Abstraction with Views  
âœ… Used **OPENROWSET** to query **Parquet files** directly from storage.  
âœ… Created **Views** for each dataset to provide an **abstraction layer**.  


#### ğŸ”¹ Comparison: Serverless vs Dedicated SQL Pools  
- **Serverless SQL Pool** â†’ Pay-as-you-go model, ideal for querying data without ETL.  
- **Dedicated SQL Pool** â†’ Pre-allocated compute resources, used for **high-performance workloads**.  

### ğŸ“š **Topics Covered**  
âœ”ï¸ Azure Synapse Analytics Features  
âœ”ï¸ Difference between Serverless & Dedicated SQL Pools  
âœ”ï¸ OPENROWSET & External Tables in Synapse  
âœ”ï¸ Querying Parquet files with SQL  

---

## ğŸ¯ **Conclusion**  
This project provided **hands-on experience** with a **full data engineering pipeline** using **Azure Services**.  
The **Medallion Architecture** allowed **efficient data ingestion, transformation, and analytics**, making data **ready for visualization**.

### ğŸ”¥ **Key Learnings**  
âœ”ï¸ End-to-end ETL Pipeline Development  
âœ”ï¸ Azure Data Factory for Data Ingestion  
âœ”ï¸ Azure Databricks for Data Transformation  
âœ”ï¸ Azure Synapse Analytics for Data Analysis  
âœ”ï¸ Medallion Architecture Implementation in Data Lakehouse 
